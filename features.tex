\yusra{As a <type of user>, I want <some feature> so that <some reason>.
Is feature request obvious/self-evident? (Yes/No)
If no, seek use cases.
Can you do it already on lsst-dev? If yes, how? If not, how much work would it be to make it possible?
Can you do it already on LSP? If yes, how? If not, how much work would it be to make it possible?
}

\subsection{All-Sky Visualization}
\yusra{TO DO: Lauren}
High on the priority list for surveys (such as LSST) with a large footprint on the sky is a tool enabling a visualization of the (to-be) observed area on the full sky covered over some time period (e.g. per night, survey-to-date).  For example, one might like to overlay the instrument footprint of all the single-epoch images accumulated to-date or in a given time period and be able to visualize their on-sky RA/Dec positions with respect to common celestial objects/reference points (e.g. the Milky Way galaxy, the Solar System and its members, constellations, Messier objects, other surveys, etc.)

Specifically, this could include functionality to both:
\begin{itemize}
  \item{display a nominal focal plane footprint at any desired location on the sky, e.g., for use in understanding planned observations; and}
  \item{display over a coadd the outlines of the single-epoch images used to generate the coadd, or of all the single-epoch images that contain a given source.}
\end{itemize}

It would also be desirable to be able to display information associated with the footprints of interest (e.g. single-epoch statistics, MAF metrics).  This would include quantities that are evaluated across the entire (LSST-observed) sky and it would be desirable to have the ability to explore them at a range of scales; from all-sky down to the finest-grained level at which they are defined.  It was noted that this becomes increasingly important in the context of LSSTCam and particularly the Science Validation surveys when larger contiguous regions of sky coverage start to emerge.

At the time of writing, such a capability does not exist specifically for LSST, although some of the desired functionality does overlap with those provided by Firefly (see, e.g. \citep{2016AAS...22734806D}).  The hscMap viewing tool developed for the HSC-SSP (see \S\ref{sec:existing_tools:hscMap}, \url{https://hscmap.mtk.nao.ac.jp/hscMap4}) provides some of this functionality for all-sky survey footprint visualization.


\subsection{Full Focal Plane Visualization}
\label{sec:large_viz}
\yusra{Thank you Scott for the draft. TO DO: John Edit}
Commissioning, Camera, and DRP expressed interest in being able to load a full focal plane and zoom continuously
from a large scale view down to pixel-level images.  In order to make pixel-level information useful, any
viewer will also need to support overplotting of mask planes.

Tony Johnson's image viewer, developed for the camera team, currently supports raft-level images and could be
extended to support full focal planes.  Luis da Costa has also developed a large scale viewer to handle preview
data as it comes off the camera.  This viewer can handle full focal planes.  There is talk of combining the
development effort on these two tools.

There are third party packages that can do large scale image
visualization, but these involve resampling images onto some dynamic pixellization of the sky (i.e.HEALPIX) and
thus, on some level, erase the information contained in individual LSST pixels.  The principal barrier to just
adopting Tony Johnson's image viewer is that it is enabled my a massive hardware investment, which may make it
prohibitive for hundreds of users to be viewing hundreds of focal planes simultaneously.

\subsection{Mark and save positions on images}

\yusra{TO DO: Yusra}

Define regions of interest.

Use Case: Crowd Sourced Image Inspection
Inspect and markup images during commissioning in order to find surfaces that need black paint (analogous to the des-exp-checker)


Showing a particular instance used by DESC for DC2 r2.1i.
Can label image data with ?problems? (e.g. tree rings) that apply at particular positions.
These are then stored to file (either just as text, or maybe a SQLite database), although it's not clear in exactly what representation.
The ?masks? shown are the actually LSST mask, rendered as opaque colour on top of the image.
It's not clear exactly which mask planes are used.
Nice UI.
Clearly a special-purpose tool.
Simple enough that it is appealing to use for appropriate tasks.
?Could use this as a simple GalaxyZoo?.
How would you stand this up for use by the DM team, e.g. for looking at diffim outputs?
Discussed how one might deploy it on e.g. LDF Kubernetes Commons at the T/CAM standup.
And authentication options.
Not appropriate for use for a single user, but could be used for crowd-sourcing visual inspection of data.
This addresses one particular set of use cases, but that set of use cases is highlighted as important in the inputs we've collected.

\subsection{Individual Image Inspection}
\label{sec:ds9}
\yusra{The title of this feature is vague.  Is this the use case supporting maintaining the ability to open images in ds9? Also note that users are most proficient with DS9.}
Independent of all other recommendations, there is still strong interest in the
ability to, outside of any notebook or JupyterLab-like environment, examine
individual raw images in detail using DS9 or some equivalent tool.  Users are
agnostic as to how this tool is provided (i.e. whether it is served through a
web browser or through some kind of terminal forwarding in the vein of xpra).
The principal concerns are that it be able to rapidly load and manipulate pixel
level data and that it be able to operate in a server-client mode so that users
do not have to download terabytes of image data onto their local machines.


\subsection{Compare Images}
\yusra{TO DO: Yusra}
\begin{itemize}
\item{blinking (animation/movie of images}
\item{side-by-side}
\item{Locking two frames by WCS or pixels coordinates}
\item{Locking two frames by scale and limits}
\item{Pan/Zoom}
\end{itemize}

All groups polled requested the ability to directly compare two images, by locking WCS or pixel coordinates, scale and limits, and both blinking and side by side.
After locking users want to be able to pan and zoom while viewing side-by-side or blinking.
Two groups also requested a ``crossfade'' option as ``nice to have'' rather than essential.
Crossfade means that two images are overlaid at the same time, and the user moves a slider to shift the relative weighting of each image.
Ideas's put forth include also include using a movable "curtain" UI element to wipe one image across the other.
\yusra{Whenever photo editing apps that let you compare the before and after of a photographs with a movable curtain, and I always wish they just let me blink.}

Using ds9 on lsst-dev this is possible and easy (for frequent users of ds9).
Using Firefly or matplotlib on Nublado, it may be possible, but it is not easy.

User stories
\begin{itemize}
\item{As a pipelines developer,  I want to be able to blink two locked images so that I can assess the effect of an algorithm modification.}
\item{As a camera team developer,  I want to be able to change the scale limits of two side-by-side images by hand, so that I can see the overscan.}
\end{itemize}


\subsection{Overlay Maps/Masks}
\yusra{Thank you Scott for the draft. TO DO: Gregory edits}

Commissioning, AP, and SST specifically requested the ability to layer images on
top of each other with varying degrees of transparency and multiple colors.
This will be used to overlay the masks (detected, saturated, interpolated, etc.)
from the LSST Science Pipelines as well as maps of known defects on top of
images during visual inspection.  The functionality should be provided through
a realtime UI for use when inspecting an image ``by hand'' and through a
programmatic API for use when inspecting an image in a JupyterLab/notebook
environment.  afwDisplay/ds9 and Firefly both currently support this
functionality.

Overlaying maps onto images (currently use afwDisplay
Toggle individual mask planes and choose colors

SST: Individual mask planes (including: across a full focal plane; for coadds)
"Individual" apparently meaning that one or more mask planes to be displayed could be selected programmatically or by UI, with flexible assignment of colors and transparency.
Princeton says: need full focal plane defect overlays at the summit
mask integration (Firefly does this well)
Obvious nans (e.g. in another color)

Colorized overlays on greyscale images (with tunable alpha)
This refers to the rendering of single-channel image data with a hue or a partially transparent color overlay based on an additional channel (e.g., colorizing a single-channel flux image by the per-pixel variance). Note that mask-overlay display could be treated as an instance of such a capability, but for tracking purposes we'll treat it separately as mask overlays (in paragraph above) which benefit from specialized UIs/APIs for extracting and naming bits from packed bit masks.


\subsection{Callbacks (Run arbitrary code on a pixel or region)}

\yusra{7/12. TO DO:  I'm not sure who is the best person to write this: Gregory John Simon}
Robert says that callbacks are important because they give the use the ability to add any feature to the image viewer. This makes the extant features available in any particular viewer less important.
Yes, If you write it in a way that you can implement arbitrary code and analysis.  Powerful. But I think in JS9 you'd have to write in javascript

Per-source drill down
Select a source/object (in a table, over plotted on an image, in an x-y plot) and be able to link to additional visualizations or data displays related to that source/object/
KB: It would be very nice to be able to click on a source (or collection of sources) in a pixel-level image and have callback to the notebook aspect to run further analysis on those sources. Basically, brushing and linking integration of pixel-level image visualization with the notebook aspect.


\subsubsection{Extensible displays}
% Move elsewhere?
\yusra{TO DO: Simon? John?}
Everything must be scriptable.
We only care that there are APIs for all these things

Because the set of interesting specific visualizations far exceeds what could be provided centrally, make it possible for users to define visualizations, constructed from the lower-level visualization capabilities in the system (or an external library), in a way that makes them straightforward to apply on demand.

\subsection{Snappy User experience}
\yusra{TO DO: Simon}
User experience:
There is a big difference between a viz platform being able to do something and having it be easy or responsive enough to not cause an extra barrier to usage.  A few specific examples of functionality that needs to be gotten right or the tool will suffer from users finding ways to use other tools:
\begin{itemize}

\item{Trivial adjustment of range and zero point of image scaling.
It should be a one click (or hotkey) operation to get a good guess at the scaling range (e.g. z-scale).
Dynamic scaling needs to be achieved through intuitive gestures
Example: When looking at raw frames, the bias has not yet been taken out.  This means each amp needs a different scaling.  When switching between amps, it must be trivial to adjust the scale parameters via the mouse so that a reasonable set of scale parameters can easily be found (ds9 is the standard for this, as far as I know).}
\item{Pan and zoom must be desktop-like
Even a small delay between a drag and the frame panning causes momentary disorientation.
Similarly, if the zoom is not close to continuous and instantaneous, the user must reorient after every change in zoom.
Example: It is common to be at a fairly high zoom level and want to pan around to find a specific type of object or feature.  If the pan takes any fraction of a second, the user needs to figure out where they are so they know where to pan again.  Similarly, one may want to zoom out to find other examples.  If the zoom isn?t smooth, requires cognitive effort to figure out a) when the right zoom has been reached and b) where to pan after the zoom.}
\item{Mouse over values need to update with ultra-low latency
The value of the pixel under the mouse needs to be displayed prominently.
The value must update with ultra-low latency  as the mouse is moved over the image.
Example 1: It is common to want to explore pixel values to get an average by eye of the background.  Just by running the mouse over pixels between objects, it is simple to get a sense of the background value if the mouse over value updates essentially continuously.
Example 2: It is common to want to try to find the ~max pixel value in an object.  Rather than drawing a trace, it is often sufficient to run the mouse over the pixels of the object to look for a gradient and follow it up.  This is much harder to do if it takes significant time to update the mouse over value.
\item{It needs to be simple to match multiple images on either image coordinates or WCS coordinates.}
Given a reference image, snap all other images in other frames to the same image or sky (configurable) coordinates and zoom level with less than three mouse clicks
Once co-registered, pan and zoom should be linked in all panes
This includes the ability to cycle through the frames in the same pane, i.e. ?blinking? through the frames sequentially via a blink selection in a menu option or hotkey.  I.e. not sequentially clicking through a series of tabs or list entries.
Example: There are lots of moving objects in images of any depth.  To find them load three epochs of the same area separated by a few days.  Register all images to the first image to the same sky coordinates.  Stack the images and in one pane and cycle through them automatically with a delay of 0.5 seconds.  Look for points sources that follow a linear trajectory relative to the surrounding stationary point sources.}
\end{itemize}

\subsection{ds9-style "smoothing" (on the fly convolutions)}
\yusra{TO DO: yusra}
Camera team are heavy users of ds9's  "smoothing."
Science Pipelines Team are also heavy users of ds9 smoothing.

\subsection{Mouseover Effects}
\yusra{TO DO: Lauren}
The ability to have a (selectable) set of values and properties displayed on mouseover of a given position is highly desirable.  Examples include (but are certainly not limited to):
\begin{itemize}
\item{Image information: id, observation date \& pointing, type, exposure time, etc.;
\item{Position of mouse in WCS (RA/Dec) and image (pixel) coords;
\item{Pixel value under mouse;}
\item{Pixel S/N under mouse (assuming a variance plane is available).}
\end{itemize}

\subsection{Dynamic stretch}
\yusra{TO DO: Simon}

Camera team visualizes raw images (still with overscan regions or amp-to-amp offset)
Interactive (and responsive) modification of color stretch parameters

Color scale
Use "Standard" image interactions in a responsive way: scale, stretch, zoom, pan, colormap and basic computation and image analysis capabilities.
There was a general desire for stretch/scale changes to be "fast".
In the meeting we discussed at least three different interpretations of color mapping: rendering of single-channel image data with a color that depends on the flux value (or whatever other variable is represented by the pixel values); rendering of single-channel image data with a hue or a partially transparent color overlay based on an additional channel; generation of color from 3 independent channels. For future purposes we'll treat this use case as referring only to the first of the three; the other two have their own use cases below.
CFC: This needs to include interactive hierarchical capabilities - e.g. display of a full FOV image consisting of 21 science rafts + 4 corner rafts in some definable spatial compression/sampling scheme (e.g. box average, max value, median filter etc..) to make the full FOV image manageable. Interactive feature need to include selecting a specific science or corner raft for more detailed display, followed by selecting a given sensor on a raft for pixel level display and computational (IRAF imexamine like) operations.
Other use cases listed here can be implemented through this one.
BS: 1 (ds9+imexam functionality)


\subsection{Display footprints and heavy footprints}
\yusra{TO DO: Gregory}
Display of LSST data objects with natural on-image interpretations: (Heavy)Footprints, PSF models at a point, PSF variation models or measurements
Footprints should be clickable to facilitate investigation into deblending computations.

Be able to viz intrinsic model (not PSF convolved) (current: matplotlib)
Subtract models from images. Be able to flip from image, model, residuals.   (current: matplotlib)

Pixel-level images with source models subtracted to see visualize residuals with respect to the model
Could be useful for large galaxies, bright stars, evaluating deblending
CFC: Could be useful for stray and scattered light analysis if the source model include ghost images from internal refections of bright sources.

\subsection{Overplot circles/ellipses/footprints}
\yusra{TO DO: Gregory}
Suggested pixel-level image visualization requirements
Ability to overlay sources, along with their model ellipses; must be able to use multiple projections, e.g., instrument, sky; must be able to pan and zoom and adjust color scale quickly; must be able to overlay masks; must be able to see map value and coordinates at mouse location; a "magnifying glass" / zoom-in inset similar to ds9 would be nice to have
CFC: This should be merged with use case 7. (LPG +1)
Overlay sources from other surveys   (current: hscMap for HSC, ????  for LSST)
Overlay source catalog with brushing and linking   (current: Firefly. Though not everyone knows how to use this.)

\subsection{Miscellaneous}
\begin{itemize}
\item{header viewer}
\item{pdf, png, jpeg output}
\item{Viewer in separate window (not just separate tab)}
\end{itemize}

\subsection{Miscellaneous requests not specific to images}
These are catalog visualization requests that augment image display, but aren't specific to displaying images in a technical sense.

\subsubsection{Brushing and linking}
\yusra{TO DO: Gregory}
Across images, histograms, and scatterplots (on the board these pairs of those were called out: image-hist, image-scatter, hist-scatter, but hist-hist, scatter-scatter, and so on also make sense). Support pairwise scatterplots (e.g., displays of the matrix of all x-y plots derivable from a set of N variables).
KB: Bokeh / holoviews / datashader offers much of this capability, for example
CFC: PyVis tools does this.
KB: Brushing and linking between matplotlib plots and python objects in a notebook is definitely important. In addition, it would be really useful to be able to examine an image (e.g., in Firefly), overlay catalog objects, select objects of interest, and then be able to access that selection from a notebook. In other words, I am asking for a linkage between pixel-level images, object catalogs, and the notebook aspect.

\subsubsection{Catalog Integration}
\yusra{TO DO: Lauren}
An obvious desire for a viewer associated with images with which any type of object catalog can be associated (e.g. catalogs specifically created for the image through image processing pipelines, external reference catalogs, etc.) would be the ability to overlay the catalogs on the image itself.  There should be a two-way ability to click on a source in an image or catalog and get associated catalog-level data or zoom to associated position on the image, respectively.  Hovering the mouse on a source should pop up useful metadata about that source.

Sub-selections of data points should be possible via mouse-clicking tools (e.g. single and shift clicking, rectangular and other geometric shapes, ``roping'') or an SQL-like querry to the database serving the catalog.  There should be numerous operations that can be executed on any seleciton of data points:
\begin{itemize}
\item{produce scatter plots and histograms of any columns in the catalog;}
\item{produce quiver plots when field vector values are provided as columns in the catalog (particularly useful in assessing calibration data, astrometric residuals, PSF moment residuals, engineering facilities measurements, etc.);}
\item{basic statistical calculations of the sample properties;}
\item{highlighting of sources with certain catalog-based characteristics (e.g. S/N threshold, flags set, etc.);}
\item{ability to save/reload any sub-selection as an external catalog.}
\end{itemize}


\subsubsection{Per-Source/Position Flipbooks}
\yusra{TO DO: Lauren}
In most cases when viewing and assessing a given image, there also exists myriad image/template types corresponding to the image of interest (and each other) in a spatial sense.  An obvious example would be the set of individual input images contributing to a given coadd image.  The ability to produce and display on-the-fly various flavors of a ``flipbook'' consisting of cutouts centered on a given source or RA/Dec from all the images related in some way to the source/position of inerest would be a great asset.  Such a flipbook would be particularly useful in the context of time-variable phenomena such as supernova (lightcurves), moving objects (trajectories), and image quality variations (e.g. seeing variability, cosmic-ray and other unmasked/interpolated cosmetic defects, high levels of extinction, etc.)  Per-source flipbooks are understood to be temporal in nature.  An automated (but also set-able) definition of cutout dimensions could be based on source properties (e.g. based on size or detection footprint, possibly of the parent for blended sources).

One could also imagine the creation of flipbooks for various categories of flux images (raw, post-ISR, calibrated, etc.) to visualize the effects of the various calibration phases.  Flipbooks of source models computed for each epoch would also be informative.  In the aid of model fitting characterization and difference imaging diagnoses in particular, flipbooks of image and model residuals (individual and/or coadd image minus templates, image-to-model residuals, where the model could be per-epoch or time-integrated) would be an invaluable quality assessment tool.
